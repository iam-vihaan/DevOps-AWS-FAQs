### AWS s3 FAQs ###

Q1. What is S3?

   Amazon S3 is a (Simple Storage Service) object storage service provided by Amazon Web Services (AWS).
   It allows you to store the data as objects like sturctured and unstructured data and retrieve any amount of data at any time from anywhere on the web. 
   S3 is designed to provide 99.999999999% (11 nines) of durability It's commonly used for backup and restore, disaster recovery, data archives, and big data analytics.
   



Q2. What are the various types of Buckets?
   
   Amazon s3 is having 3 different types of buckets those are as below
  
    a. general purpose bucket: 
      
      *  General purpose buckets are the original S3 bucket type, and a single general purpose bucket can contain objects stored across all storage classes except S3 Express One Zone.
      *  They are recommended for Data Storage for Application, Backup and Archiving, Store logs, events, and telemetry data for monitoring, troubleshooting,
         
   
   b. S3 directory bucket 
     
      *  S3 directory buckets only allow objects stored in the S3 Express One Zone storage class, which provides faster data processing within a single Availability Zone.
      *  They are recommended for low-latency use cases. Each S3 directory bucket can support hundreds of thousands of transactions per second (TPS), independent of the number of directories within the bucket.
   
   c.  S3 table bucket
      
      *  A table bucket is purpose-built for storing tables using the Apache Iceberg format.
      *  S3 table buckets are specifically optimized for analytics and machine learning workloads. With built-in support for Apache Iceberg
      *  you can query tabular data in S3 with popular query engines including Amazon Athena, Amazon Redshift, and Apache Spark.
      *  Use S3 table buckets to store tabular data such as daily purchase transactions, streaming sensor data.
   





Q3. How Amazon ensure HA for S3?
   
   Amazon S3 ensures high availability( 99.99%) through several key strategies and features it.

   S3 stores data across multiple Availability Zones (AZs) within a region. These AZs are physically separated and isolated, providing redundancy and fault tolerance.
   S3 supports cross-region replication (CRR) and same-region replication (SRR), allowing you to automatically replicate objects across different AWS regions or within the same region for added resilience.
   S3 versioning keeps multiple versions of an object in the same bucket, enabling you to recover from accidental deletions or overwrites.



Q4. What is S3 Versioning and explain different States?

   Amazon S3 Versioning is a feature that allows you to keep multiple versions of an object in the same bucket.  This helps you recover from unintended user actions and application failures. 
   When versioning is enabled, Amazon S3 automatically assigns a unique version ID to each object stored in the bucket.
   
   There are three states for S3 Versioning:
    
   * Unversioned (Default): This is the default state when you create a bucket. In this state, versioning is not enabled, and only the latest version of an object is stored.
  
   * Versioning-Enabled: When you enable versioning on a bucket, S3 starts keeping multiple versions of an object. If you delete an object, S3 inserts a delete marker instead of removing the object permanently. 
                         If you overwrite an object, S3 keeps the previous version as a noncurrent version, allowing you to restore it if needed
    
   * Versioning-Suspended: In this state, versioning is suspended, meaning S3 stops creating new versions of objects. However, existing versions are retained, and you can still access them. 
                           New objects added to the bucket will not have version IDs


Example:  aws s3api put-bucket-versioning --bucket my-bucket --versioning-configuration Status=Suspended  
          aws s3api put-bucket-versioning --bucket my-bucket --versioning-configuration Status=enabled



Q5. What does a lifecycle rule consists of ?
 
    
    An Amazon S3 lifecycle rule consists of several key elements that define how S3 manages objects over their lifecycle.
    
    Rule ID: A unique identifier for the lifecycle rule.
    Filter: Specifies the subset of objects to which the rule applies. This can be based on a prefix, tags, or a combination of both.
    Status: Indicates whether the rule is enabled or disabled.
    Transition Actions: Define when objects transition to another storage class. For example, you might transition objects to S3 Standard-IA after 30 days or to S3 Glacier after 365 days.
    Expiration Actions: Specify when objects expire and are permanently deleted. For example, you might set objects to expire 365 days after creation1.
    Noncurrent Version Expiration: Specifies when noncurrent versions of objects expire if versioning is enabled.
    Abort Incomplete Multipart Uploads: Defines the number of days after the initiation of a multipart upload when S3 should abort the upload if it is not complete.


   ##lifecycle rule##
            {
  "Rules": [
    {
      "ID": "Transition and Expiration Rule",
      "Filter": {
        "Prefix": "logs/"
      },
      "Status": "Enabled",
      "Transitions": [
        {
          "Days": 30,
          "StorageClass": "STANDARD_IA"
        },
        {
          "Days": 365,
          "StorageClass": "GLACIER"
        }
      ],
      "Expiration": {
        "Days": 730
      },
      "NoncurrentVersionExpiration": {
        "NoncurrentDays": 365
      },
      "AbortIncompleteMultipartUpload": {
        "DaysAfterInitiation": 7
      }
    }
  ]
}







Q6. How to set an S3 Lifecycle configuration using AWS CLI ?


   Create a file named lifecycle.json with the following content:
  
   aws s3api put-bucket-lifecycle-configuration --bucket your-bucket-name --lifecycle-configuration file://lifecycle.json

  
   {
  "Rules": [
    {
      "ID": "Transition and Expiration Rule",
      "Filter": {
        "Prefix": "logs/"
      },
      "Status": "Enabled",
      "Transitions": [
        {
          "Days": 30,
          "StorageClass": "STANDARD_IA"
        },
        {
          "Days": 365,
          "StorageClass": "GLACIER"
        }
      ],
      "Expiration": {
        "Days": 730
      },
      "NoncurrentVersionExpiration": {
        "NoncurrentDays": 365
      },
      "AbortIncompleteMultipartUpload": {
        "DaysAfterInitiation": 7
      }
    }
  ]
}









Q7. How would you prevent versioning-related performance degradation issues?


    To prevent performance degradation issues related to S3 versioning, you can follow these best practices:

    Implement lifecycle policies to manage the number of versions stored. For example, you can create a rule to delete noncurrent versions after a certain number of days, reducing the number of versions that need to be managed
    Transition older versions to cheaper storage classes (e.g., Glacier) or delete them after a certain period.
    Apply versioning only to buckets or objects where it is necessary. Avoid enabling versioning on buckets with high churn rates unless absolutely needed
    Use AWS CloudWatch to monitor the performance of your S3 buckets and set up alerts for any unusual activity or performance issues
    Use appropriate storage classes for different versions of objects. For example, transition older versions to cheaper storage classes like S3 Glacier or S3 Glacier Deep Archive
    Use S3 Batch Operations to perform large-scale deletions or transitions of object versions efficiently


Q8. What are the different Storage classes in S3 ?

    Amazon S3 offers several storage classes to help you optimize costs and performance based on your data access patterns. Here are the main storage classes:

    S3 Standard: For frequently accessed data. It offers high durability, availability, and performance.
    S3 Intelligent-Tiering: Automatically moves data between two access tiers (frequent and infrequent) when access patterns change, optimizing costs.
    S3 Standard-Infrequent Access (S3 Standard-IA): For data that is accessed less frequently but requires rapid access when needed.
    S3 One Zone-Infrequent Access (S3 One Zone-IA): Similar to S3 Standard-IA but stores data in a single Availability Zone, offering lower cost.
    S3 Glacier Instant Retrieval: For archive data that needs immediate access.
    S3 Glacier Flexible Retrieval: For long-term archive data that is rarely accessed and can tolerate retrieval times of minutes to hours.
    S3 Glacier Deep Archive: For long-term data archiving with retrieval times of 12 hours or more, at the lowest cost. 





Q9. How Amazon S3 manage costs, meet regulatory requirements, reduce latency?
  
    Amazon S3 employs several strategies to manage costs, meet regulatory requirements, and reduce latency:
    
    Managing Costs:

    S3 offers various storage classes like S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, and S3 Glacier, each optimized for different access patterns and cost requirements.
    You can define lifecycle policies to automatically transition objects to more cost-effective storage classes or delete them after a specified period1.
    This storage class automatically moves data between frequent and infrequent access tiers based on changing access patterns, optimizing costs without performance impact.
    Cost Management Tools: AWS provides tools like AWS Cost Explorer and AWS Budgets to monitor and manage your S3 costs effectively.

    Meeting Regulatory Requirements:
 
   
    Data Encryption: S3 supports both server-side encryption (SSE) and client-side encryption to protect data at rest.
    Object Lock: This feature allows you to store objects using a write-once-read-many (WORM) model, helping meet regulatory requirements for data immutability.
    AWS Artifact: Provides on-demand access to AWS compliance reports and certifications5.

    Reducing Latency:

    S3 Transfer Acceleration: Uses Amazon CloudFront's globally distributed edge locations to accelerate data transfers over long distances.
    Optimized Bucket Configuration: Using multiple prefixes and parallelization can help achieve higher request rates and reduce latency.
    Caching: Integrating Amazon CloudFront or Amazon ElastiCache with S3 can help reduce latency by caching frequently accessed data.
    Proximity to Compute Resources: Placing S3 buckets in the same region as your compute resources (e.g., Amazon EC2) can minimize latency.




Q10. Why Amazon S3 is the best backend to store your state file ?


     Amazon S3 is considered one of the best backends for storing state files, especially for infrastructure as code tools like Terraform, due to several key reasons:
     
     Amazon S3 is designed for 99.999999999% (11 nines) durability and 99.99% availability over a given year1. This ensures that your state files are highly resilient and always accessible.
     Security: S3 offers robust security features, including encryption at rest and in transit, fine-grained access control using AWS IAM policies, and integration with AWS CloudTrail for logging and monitoring access
     Versioning: S3 supports versioning, which allows you to keep multiple versions of your state files. This is crucial for recovering from accidental deletions or changes
     State Locking: When used with DynamoDB, S3 can provide state locking and consistency checking, preventing concurrent modifications and ensuring the integrity of your state files
     Cost-Effectiveness: S3 offers various storage classes that can help optimize costs based on your access patterns. For example, you can use S3 Standard for frequently accessed state files and S3 Glacier for long-term storage

# example-backend:

terraform {
  backend "s3" {
    bucket         = "techopsbucket123"
    key            = "vpcec2/terraform.tfstate"
    region         = "us-east-1"
    encrypt        = true
    dynamodb_table = "demo_table123"
  }
}



Q11. Sample S3 bucket that follows industry best practices for security, data protection, and management.
 

resource "aws_s3_bucket" "foo-bucket" {

  region        = var.region
  bucket        = local.bucket_name
  force_destroy = true
 
  tags = {
    Name = "foo-${data.aws_caller_identity.current.account_id}"
  }

  versioning {
    enabled = true
  }

  logging {
    target_bucket = "${aws_s3_bucket.log_bucket.id}"
    target_prefix = "log/"
  }

  server_side_encryption_configuration {
    rule {
      apply_server_side_encryption_by_default {
        kms_master_key_id = "${aws_kms_key.mykey.arn}"
        sse_algorithm     = "aws:kms"
      }
    }
  }
  acl    = "private"
}
 


Region: Specifies the AWS region where the bucket will be created.
Bucket Name: Uses a local variable for the bucket name.
Force Destroy: Allows the bucket to be destroyed even if it contains objects.
Tags: Adds tags to the bucket for identification and management.
Versioning: Enables versioning to keep multiple versions of objects.
Logging: Configures server access logging to a target bucket with a specified prefix.
Server-Side Encryption: Uses AWS KMS for server-side encryption with a specified KMS key.
ACL: Sets the bucket's access control list to private.



Q12. what are the Key S3 Metrics Available in CloudWatch?


     These metrics are available by default for all S3 buckets.

     1. Storage Metrics:

        BucketSizeBytes: The amount of data in bytes stored in a bucket, including all storage classes.
        NumberOfObjects: The total number of objects stored in a bucket.
     
     2. Request Metrics:

        AllRequests: The total number of HTTP requests made to a bucket.
	  GetRequests: The number of HTTP GET requests made to a bucket.
	  PutRequests: The number of HTTP PUT requests made to a bucket.
	  DeleteRequests: The number of HTTP DELETE requests made to a bucket.
	  HeadRequests: The number of HTTP HEAD requests made to a bucket.
	  PostRequests: The number of HTTP POST requests made to a bucket.
 	  ListRequests: The number of HTTP LIST requests made to a bucket.

    3. Data Transfer Metrics:
  
       BytesDownloaded: The number of bytes downloaded from a bucket.
       BytesUploaded: The number of bytes uploaded to a bucket1.

    4. Replication Metrics:
   
       ReplicationPendingOperations: The total number of S3 API operations pending replication.
       ReplicationFailedOperations: The total number of S3 API operations that failed replication.
       ReplicationLatency: The maximum replication time to the destination AWS Region
    
    5. Lifecycle Metrics:

       TransitionedObjectCount: The number of objects transitioned to another storage class1.
       ExpiredObjectDeleteMarkerCount: The number of delete markers created by lifecycle expiration rules 


4xxErrors: The number of requests that resulted in client-side errors (e.g., 400, 404).
5xxErrors: The number of requests that resulted in server-side errors (e.g., 500, 503).


Example:

aws s3api put-bucket-metrics-configuration \
    --bucket my-bucket \
    --id my-metrics-config \
    --metrics-configuration '{
        "Id": "EntireBucket",
        "Filter": {},
        "Metrics": {
            "IsEnabled": true
        }
    }'


   




Q13. How does S3 handle access logging?

     Amazon S3 handles access logging through a feature called Server Access Logging. This feature provides detailed records for the requests made to your S3 bucket. Hereâ€™s how it works:

     1. Enabling Logging: You can enable server access logging via the S3 console, AWS CLI, SDKs, or API. When enabled, 
                          S3 logs all access requests to a specified destination bucket (also known as a target bucket) in the same AWS region.

     2. Log Delivery: S3 delivers access logs to the destination bucket in a specific format. Each log record contains details about the request.
        such as the request type, the resources specified, and the time and date of the request2.
    
     3. Log Content: Access logs include information like:

         The bucket name
	   The requester's IP address
	   The request time
 	   The request type (e.g., GET, PUT)
   	   The HTTP status code of the response
   	   The size of the object returned
  
     4. Permissions: To enable log delivery, you must grant the logging service principal (logging.s3.amazonaws.com) the necessary permissions to write logs to the destination bucket. 
                     This can be done using a bucket policy or access control lists (ACLs

     5. Cost: There is no extra charge for enabling server access logging, but you will incur the usual charges for storing the log files
 
     By using server access logging, you can gain valuable insights into the usage patterns of your S3 buckets, which can be useful for security audits, access audits, and understanding your S3 billing.


step-by-step guide to enable S3 access logging using the AWS CLI:

1.  aws s3api create-bucket --bucket my-log-bucket --region us-west-2  -- create target bucket
2.  vim bucket-policy.json     -- create bucket policy
      {
   "Version": "2012-10-17",
   "Statement": [
     {
       "Effect": "Allow",
       "Principal": {"Service": "logging.s3.amazonaws.com"},
       "Action": "s3:PutObject",
       "Resource": "arn:aws:s3:::my-log-bucket/*"
     }
   ]
 }
 
3.  aws s3api put-bucket-policy --bucket my-log-bucket --policy file://bucket-policy.json  -- apply bucket policy
4. aws s3api put-bucket-logging --bucket my-source-bucket --bucket-logging-status   -- enabling the loging for source bucket
      '{
  	"LoggingEnabled": {
  	  "TargetBucket": "my-log-bucket",
   	 "TargetPrefix": "my-source-bucket-logs/"
 	 }
	}'

   
   
   


Q14. What are presigned URLs?


     Presigned URLs in Amazon S3 are URLs that grant temporary access to objects in your S3 bucket without requiring AWS credentials. 
     They are particularly useful for sharing private objects securely.

     1. Generation: A presigned URL is generated using AWS SDKs or the AWS CLI. When you create a presigned URL, you specify the bucket name, object key, HTTP method (GET, PUT, etc.), and an expiration time

     2. Temporary Access: The URL includes a signature that grants temporary access to the specified object. The expiration time determines how long the URL remains valid.
    
     3. Use Cases:
         
           Sharing Files: Share private files with users without making the objects publicly accessible.
	     Uploading Files: Allow users to upload files directly to your S3 bucket without needing AWS credentials.
	     Temporary Access: Provide temporary access to objects for specific use cases, such as downloading a report or accessing a time-sensitive resource

     4. Security: Presigned URLs are secure because they include a signature that is valid only for the specified duration. Once the URL expires, it can no longer be used to access the object.




Q15. What is S3 Select and how is it used?


     How it works:

     Amazon S3 Select is a feature that allows you to retrieve a subset of data from an S3 object using SQL queries.
     This can significantly reduce the amount of data transferred and improve query performance by filtering the data at the source
 
     Usecases:
  
     Data Analysis: Quickly extract specific data from large datasets without needing to download the entire object.
     Cost Reduction: Reduce data transfer costs by retrieving only the necessary data.
     Performance Improvement: Speed up data processing by filtering data at the source.

    Example:

    aws s3api select-object-content \
      --bucket your-bucket-name \
      --key sales-data.csv \
      --expression "SELECT * FROM S3Object WHERE product = 'ProductA'" \
      --expression-type SQL \
      --input-serialization '{"CSV": {"FileHeaderInfo": "USE"}}' \
      --output-serialization '{"CSV": {}}' \
      output.csv





Q16. What is Cross-Region Replication (CRR) in S3?


     Cross-Region Replication (CRR) in Amazon S3 is a feature that automatically replicates objects from one S3 bucket to another bucket located in a different AWS region.
     This helps improve data availability, durability, and compliance with regulatory requirements.
  
     Use Cases:
        
          Disaster Recovery: Maintain a copy of your data in a different region to protect against regional outages.
          Latency Reduction: Provide lower-latency access to data for users in different geographic regions

CRR implementation:

step1: enable versioning for both source and destination buckets

1. aws s3api put-bucket-versioning --bucket source-bucket --versioning-configuration Status=Enabled   
2. aws s3api put-bucket-versioning --bucket destination-bucket --versioning-configuration Status=Enabled

step2: create policy file, create role and attach policy,configure replication

1. vim replication-policy.json

    {
   "Version": "2012-10-17",
   "Statement": [
     {
       "Effect": "Allow",
       "Principal": {"Service": "s3.amazonaws.com"},
       "Action": "sts:AssumeRole"
     },
     {
       "Effect": "Allow",
       "Action": "s3:*",
       "Resource": [
         "arn:aws:s3:::source-bucket/*",
         "arn:aws:s3:::destination-bucket/*"
       ]
     }
   ]
 }
 
2.  aws iam create-role --role-name replication-role --assume-role-policy-document file://replication-policy.json 
3.  aws iam put-role-policy --role-name replication-role --policy-name replication-policy --policy-document file://replication-policy.json
4.  aws s3api put-bucket-replication --bucket source-bucket --replication-configuration '{
   "Role": "arn:aws:iam::account-id:role/replication-role",
   "Rules": [
     {
       "Status": "Enabled",
       "Prefix": "",
       "Destination": {
         "Bucket": "arn:aws:s3:::destination-bucket"
       }
     }
   ]
 }'





      
Q17. Explain S3 Access Grants.


     S3 Access Grants simplifies the process of managing S3 permissions. Making it easier to enforce security policies 
     and ensure that users have the appropriate level of access to the data they need.


     Key Features of S3 Access Grants:


     Granular Permissions: Define access permissions at the prefix, bucket, or object level, allowing for fine-grained control over who can access specific data
     Integration with Identity Providers: Supports integration with popular identity providers like Entra ID, Okta, Ping, and OneLogin, enabling seamless authentication and authorization
     Auditability: Enhanced integration with AWS CloudTrail logs end-user identity and the application used to access S3 data, providing detailed audit history
     Scalability: Supports up to 100,000 grants per region per account, allowing you to scale permissions management efficiently

     Example Use Case:

     Imagine you have a data lake in S3 and need to grant access to different teams within your organization. With S3 Access Grants, you can:

        Map specific S3 prefixes to user groups in your corporate directory.
        Define read-only, write-only, or read-write access for each group.
        Automatically log and audit all access requests to ensure compliance and security.

Example:

1. aws s3control create-access-grants-instance \             --- Create an S3 Access Grants Instance
    --account-id 123456789012 \
    --region us-west-2

2. aws s3control register-location \                         --- Register a Location 
    --instance-id your-instance-id \
    --location s3://example-bucket/specific-prefix/


3. aws s3control create-grant \                              --- Create a grant
    --instance-id your-instance-id \
    --location s3://example-bucket/specific-prefix/ \
    --grantee corporate-directory-group \
    --permissions READ


4. aws s3control request-temporary-credentials \             --- Request temporary credentials
    --instance-id your-instance-id \
    --grantee corporate-directory-group








Q18. HTTP status codes

     HTTP status codes are standardized codes that indicate the result of a client's request to a server. They are grouped into five categories:

     1. 1xx: Informational:
       
        * 100 Continue: The server has received the request headers and the client should proceed to send the request body.
        * 101 Switching Protocols: The requester has asked the server to switch protocols and the server has agreed to do so.
     
     2. 2xx: Success:

        * 200 OK: The request was successful.
	  * 201 Created: The request was successful and a new resource was created.
	  * 202 Accepted: The request has been accepted for processing, but the processing is not complete.
	  * 204 No Content: The server successfully processed the request, but is not returning any content.
    
    3. 3xx: Redirection:

        * 301 Moved Permanently: The resource has been moved to a new URL permanently.
	  * 302 Found: The resource has been temporarily moved to a different URL.
        * 304 Not Modified: The resource has not been modified since the last request.

    4. 4xx: Client Errors:

       * 400 Bad Request: The server could not understand the request due to invalid syntax.
	 * 401 Unauthorized: The client must authenticate itself to get the requested response.
       * 403 Forbidden: The client does not have access rights to the content.
       * 404 Not Found: The server cannot find the requested resource.
       * 429 Too Many Requests: The user has sent too many requests in a given amount of time.

    5. 5xx: Server Errors:

       * 500 Internal Server Error: The server encountered an unexpected condition that prevented it from fulfilling the request.
       * 502 Bad Gateway: The server received an invalid response from the upstream server.
	 * 503 Service Unavailable: The server is not ready to handle the request, often due to maintenance or overload.
       * 504 Gateway Timeout: The server did not receive a timely response from the upstream server.

   
       
     